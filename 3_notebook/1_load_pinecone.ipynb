{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-05T23:30:48.394322Z",
     "start_time": "2024-10-05T23:30:48.388116Z"
    }
   },
   "source": [
    "import asyncio\n",
    "import os\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableBranch, RunnablePassthrough\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_cohere import ChatCohere\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_pinecone import Pinecone as LangchainPinecone\n",
    "from nemoguardrails.integrations.langchain.runnable_rails import RunnableRails\n",
    "\n",
    "from nemoguardrails.actions import action\n",
    "import json\n",
    "from utils.util import (load_environment_variables,\n",
    "                        initialize_clients,\n",
    "                        create_embeddings,\n",
    "                        create_pinecone_collection)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T23:30:49.371685Z",
     "start_time": "2024-10-05T23:30:49.368575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def get_file_path(filename):\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    return os.path.join(script_dir, filename)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n"
   ],
   "id": "aff673c7ee1e6b3b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T23:30:50.322388Z",
     "start_time": "2024-10-05T23:30:50.313653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from nemoguardrails import LLMRails\n",
    "from nemoguardrails.embeddings.basic import EmbeddingModel\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class CohereEmbeddingModel(EmbeddingModel):\n",
    "    \"\"\"Cohere embedding model.\"\"\"\n",
    "\n",
    "    engine_name = \"cohere\"\n",
    "\n",
    "    #embedding_model=\"embed-english-v3.0\", api_key=env_vars[\"cohere_api_key\"]\n",
    "    def __init__(self, embedding_model: str = \"embed-english-v3.0\"):\n",
    "        \"\"\"Initialize the Cohere embedding model.\"\"\"\n",
    "        #super().__init__()\n",
    "        print(\"initating embedding model\")\n",
    "        self.cohere_embeddings = CohereEmbeddings(\n",
    "            model=embedding_model,\n",
    "            cohere_api_key=os.getenv(\"COHERE_API_KEY\")\n",
    "        )\n",
    "\n",
    "    async def encode_async(self, documents: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Encode the given documents asynchronously.\"\"\"\n",
    "        return self.cohere_embeddings.embed_documents(documents)\n",
    "\n",
    "    def encode(self, documents: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Encode the given documents.\"\"\"\n",
    "        return self.cohere_embeddings.embed_documents(documents)\n",
    "\n",
    "def init(app: LLMRails):\n",
    "    print(\"init LLMRails model\")\n",
    "    app.register_embedding_provider(CohereEmbeddingModel, \"cohere\")\n"
   ],
   "id": "d02f20d8f83d41b2",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T23:30:51.226590Z",
     "start_time": "2024-10-05T23:30:51.222413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "colang_content = \"\"\"\n",
    "define user greeting\n",
    "  \"Hi\"\n",
    "  \"Hello,how are you\"\n",
    "  \"Can you help me\"\n",
    "\n",
    "define bot greeting\n",
    "  \"Hello,how may I assist you today\"\n",
    "  \"Hello\"\n",
    "\n",
    "define user ask politics\n",
    "  \"what do you think about the new president?\"\n",
    "  \"who do you support in current elections\"\n",
    "  \"what are your political beliefs?\"\n",
    "  \"thoughts on the president?\"\n",
    "  \"left wing\"\n",
    "  \"right wing\"\n",
    "\n",
    "define bot answer politics\n",
    "  \"I am sorry,I only answer questions on deep learning, I do not make political statements\"\n",
    "\n",
    "define flow politics\n",
    "  user ask politics\n",
    "  bot answer politics\n",
    "  bot offer_help\n",
    "\n",
    "define user ask deeplearning\n",
    "  \"what is deep learning\"\n",
    "  \"what is a CNN\"\n",
    "  \"what is a RNN\"\n",
    "  \"what is CML\"\n",
    "  \"what is model training\"\n",
    "\n",
    "define flow deeplearning\n",
    "  bot greeting\n",
    "  user ask deeplearning\n",
    "  $answer = execute qa_chain(query=$last_user_message)\n",
    "  bot $answer\n",
    "\n",
    "\n",
    "define user ask programming\n",
    "  \"write a python code\"\n",
    "  \"can you code\"\n",
    "  \"programming question\"\n",
    "  \"write a python code ?\"\n",
    "\n",
    "define bot answer programming\n",
    "  \"While I can discuss programming concepts related to deep learning, I'm not designed to write or execute code. If you have questions about programming in the context of deep learning, I'd be happy to help with that. What specific aspect of programming in deep learning would you like to know about?\"\n",
    "\n",
    "define flow programming\n",
    "  user ask programming\n",
    "  bot answer programming\n",
    "  bot offer_help\n",
    "\n",
    "\"\"\"\n",
    "yaml_content = \"\"\"\n",
    "models:\n",
    " - type: main\n",
    "   engine: cohere\n",
    "   model: command-r-plus\n",
    " - type: embeddings\n",
    "   engine: cohere\n",
    "   model: embed-english-v3.0\n",
    "\"\"\""
   ],
   "id": "b63f561f8d8797dc",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T23:30:54.093571Z",
     "start_time": "2024-10-05T23:30:52.729350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "env_vars = load_environment_variables()\n",
    "index_name = env_vars[\"pinecone_index\"]\n",
    "cohere_client, pinecone_client = initialize_clients(env_vars[\"cohere_api_key\"], env_vars[\"pinecone_api_key\"])\n",
    "embeddings = create_embeddings(env_vars[\"cohere_api_key\"])\n",
    "index = create_pinecone_collection(pinecone_client, index_name)\n",
    "model = ChatCohere(model=\"command-r-plus\", temperature=0)\n",
    "\n",
    "rails_config = RailsConfig.from_content(\n",
    "    colang_content=colang_content,\n",
    "    yaml_content=yaml_content\n",
    ")\n",
    "\n",
    "rag_rails = LLMRails(rails_config, verbose=False)\n",
    "rag_rails.register_embedding_provider(CohereEmbeddingModel, \"cohere\")\n",
    "\n",
    "rag_template = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are a helpful assistant. Use the following pieces of context to answer the question at the end. \n",
    "        Answer always start with [Naga:]. If you don't know the answer, just say that I am sorry. \n",
    "        You are a helpful assistant offering further help. \n",
    "        Don't try to make up an answer.\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\"\"\"\n",
    ")\n",
    "\n",
    "docsearch = LangchainPinecone.from_existing_index(index_name=index_name, embedding=embeddings)\n",
    "\n",
    "def create_documents(input_dict):\n",
    "    question = input_dict[\"question\"]\n",
    "    docs = docsearch.similarity_search(question)\n",
    "    context = format_docs(docs)\n",
    "    return {\"context\": context, \"question\": question}\n",
    "\n",
    "def print_full_prompt(prompt):\n",
    "    print(\"Full Generated Prompt:\")\n",
    "    print(\"----------------------\")\n",
    "    print(prompt)\n",
    "    print(\"----------------------\")\n",
    "    return prompt\n",
    "\n",
    "rag_chain = (\n",
    "            RunnablePassthrough()\n",
    "            | create_documents\n",
    "            | rag_template\n",
    "            | print_full_prompt  # Add this step to print the full prompt\n",
    "            | model\n",
    "            | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_rails.register_action(rag_chain, name=\"qa_chain\")\n",
    "\n",
    "print(\"done\")\n"
   ],
   "id": "3bbcca87c02d688a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-05 19:30:52 - util.py - Creating 1024-dimensional index called 'ml-docs'...\n",
      "2024-10-05 19:30:52 - util.py - Index already exists, continuing...\n",
      "2024-10-05 19:30:53 - util.py - Checking Pinecone for active indexes...\n",
      "2024-10-05 19:30:53 - util.py - Getting description for 'ml-docs'...\n",
      "2024-10-05 19:30:53 - util.py - Getting 'ml-docs' as object...\n",
      "2024-10-05 19:30:53 - util.py - Success\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active indexes: {'indexes': [{'deletion_protection': 'disabled',\n",
      "              'dimension': 1024,\n",
      "              'host': 'ml-docs-04fgi1r.svc.aped-4627-b74a.pinecone.io',\n",
      "              'metric': 'cosine',\n",
      "              'name': 'ml-docs',\n",
      "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
      "              'status': {'ready': True, 'state': 'Ready'}},\n",
      "             {'deletion_protection': 'disabled',\n",
      "              'dimension': 1024,\n",
      "              'host': 'customer-reviews-1-04fgi1r.svc.aped-4627-b74a.pinecone.io',\n",
      "              'metric': 'cosine',\n",
      "              'name': 'customer-reviews-1',\n",
      "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
      "              'status': {'ready': True, 'state': 'Ready'}}]}\n",
      "Description: {'deletion_protection': 'disabled',\n",
      " 'dimension': 1024,\n",
      " 'host': 'ml-docs-04fgi1r.svc.aped-4627-b74a.pinecone.io',\n",
      " 'metric': 'cosine',\n",
      " 'name': 'ml-docs',\n",
      " 'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
      " 'status': {'ready': True, 'state': 'Ready'}}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cohere already exists in the registry",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 14\u001B[0m\n\u001B[1;32m      8\u001B[0m rails_config \u001B[38;5;241m=\u001B[39m RailsConfig\u001B[38;5;241m.\u001B[39mfrom_content(\n\u001B[1;32m      9\u001B[0m     colang_content\u001B[38;5;241m=\u001B[39mcolang_content,\n\u001B[1;32m     10\u001B[0m     yaml_content\u001B[38;5;241m=\u001B[39myaml_content\n\u001B[1;32m     11\u001B[0m )\n\u001B[1;32m     13\u001B[0m rag_rails \u001B[38;5;241m=\u001B[39m LLMRails(rails_config, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m---> 14\u001B[0m \u001B[43mrag_rails\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mregister_embedding_provider\u001B[49m\u001B[43m(\u001B[49m\u001B[43mCohereEmbeddingModel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcohere\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m rag_template \u001B[38;5;241m=\u001B[39m PromptTemplate\u001B[38;5;241m.\u001B[39mfrom_template(\n\u001B[1;32m     17\u001B[0m \u001B[38;5;250m        \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;124;03m        You are a helpful assistant. Use the following pieces of context to answer the question at the end. \u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;124;03m        Answer:\"\"\"\u001B[39;00m\n\u001B[1;32m     28\u001B[0m )\n\u001B[1;32m     30\u001B[0m docsearch \u001B[38;5;241m=\u001B[39m LangchainPinecone\u001B[38;5;241m.\u001B[39mfrom_existing_index(index_name\u001B[38;5;241m=\u001B[39mindex_name, embedding\u001B[38;5;241m=\u001B[39membeddings)\n",
      "File \u001B[0;32m~/Desktop/ak/naga/workspace/cohere-nemo-demo/.venv/lib/python3.9/site-packages/nemoguardrails/rails/llm/llmrails.py:1106\u001B[0m, in \u001B[0;36mLLMRails.register_embedding_provider\u001B[0;34m(self, cls, name)\u001B[0m\n\u001B[1;32m   1093\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mregister_embedding_provider\u001B[39m(\n\u001B[1;32m   1094\u001B[0m     \u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mcls\u001B[39m: Type[EmbeddingModel], name: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1095\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1096\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Register a custom embedding provider.\u001B[39;00m\n\u001B[1;32m   1097\u001B[0m \n\u001B[1;32m   1098\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1104\u001B[0m \u001B[38;5;124;03m        ValueError: If the model does not have 'encode' or 'encode_async' methods.\u001B[39;00m\n\u001B[1;32m   1105\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1106\u001B[0m     \u001B[43mregister_embedding_provider\u001B[49m\u001B[43m(\u001B[49m\u001B[43mengine_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ak/naga/workspace/cohere-nemo-demo/.venv/lib/python3.9/site-packages/nemoguardrails/embeddings/providers/__init__.py:56\u001B[0m, in \u001B[0;36mregister_embedding_provider\u001B[0;34m(model, engine_name)\u001B[0m\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     52\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe engine name must be provided either in the model or as an argument.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     53\u001B[0m     )\n\u001B[1;32m     55\u001B[0m registry \u001B[38;5;241m=\u001B[39m EmbeddingProviderRegistry()\n\u001B[0;32m---> 56\u001B[0m \u001B[43mregistry\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mengine_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ak/naga/workspace/cohere-nemo-demo/.venv/lib/python3.9/site-packages/nemoguardrails/registry.py:46\u001B[0m, in \u001B[0;36mRegistry.add\u001B[0;34m(self, name, item)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Add an item to the registry.\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \n\u001B[1;32m     38\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;124;03m    ValueError: If the item name already exists in the registry.\u001B[39;00m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems:\n\u001B[0;32m---> 46\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m already exists in the registry\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menable_validation:\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalidate(name, item)\n",
      "\u001B[0;31mValueError\u001B[0m: cohere already exists in the registry"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T23:31:21.634785Z",
     "start_time": "2024-10-05T23:30:55.613953Z"
    }
   },
   "cell_type": "code",
   "source": "await rag_rails.generate_async(prompt=\"Write about CML ?\")",
   "id": "65c7069c463d5680",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'd be happy to help!\\nContinuous Integration and Continuous Deployment (CI/CD) are essential practices in modern software development, and they play a crucial role in Machine Learning (ML) and Deep Learning (DL) as well. CI/CD enables developers to automate the testing, integration, and deployment of their code changes, ensuring that the ML/DL models are trained and deployed efficiently and reliably.\\nOne key benefit of CI/CD in the context of ML/DL is the ability to catch issues early in the development cycle. By automating the testing and validation process, developers can identify and fix bugs or incompatibilities before they cause major problems. This is especially important in ML/DL projects, where small changes in the code or data can have significant impacts on the model's performance.\\nAnother advantage of CI/CD is the ability to deploy models quickly and consistently. With automated deployment pipelines, developers can easily roll out updates to production environments, ensuring that end-users always have access to the latest and most accurate models. This is crucial in fields such as healthcare, finance, and autonomous systems, where model accuracy and reliability are of utmost importance.\\nAdditionally, CI/CD practices promote collaboration and reproducibility in ML/DL projects. By having\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "38a31134cdc912b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
